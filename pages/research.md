---
layout: page
title: Research
description: Bongjun Kim's research
---

Currently, I am working on an interactive intelligent interface and AI for sound. Specifically, I am interested in how to reduce human annotation effort for both manual sound event annotation and building automated sound event detection systems. In the past, I have worked on musical interfaces, computer-generated music, and interactive media art. 

<hr>

<a href="{{ BASE_PATH }}/pages/research.html"><img src="{{ BASE_PATH }}/pages/files/eusipco19.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Self-supervised Attention Model for Weakly Labeled Audio Event Classification](https://bongjun.github.io/pages/research.html)
We describe a novel weakly labeled Audio Event Classification approach based on a self-supervised attention model.


<br>
<a href="{{ BASE_PATH }}./pages/research.html"><img src="{{ BASE_PATH }}/pages/files/imitation_feedback_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Improving Content-based Audio Retrieval by Vocal Imitation Feedback](https://bongjun.github.io/pages/research.html)

You can improve audio search resultsjust by imitating sound events you do or do *not* want in the search result. [[paper]]({{ BASE_PATH }}/pages/files/papers/icassp19_Kim.pdf) <br><br>

<br>
<a href="{{ BASE_PATH }}/pages/projects/ised.html"><img src="{{ BASE_PATH }}/pages/files/ised_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [A Human-in-the-loop System for Sound Event Detection and Annotation]({{ BASE_PATH }}/pages/projects/ised.html)

The human-machine collaboration speeds up searching for sound events of interest within a lengthy audio recording. [[read more]]({{ BASE_PATH }}/pages/projects/ised.html)<br><br>

<br>
<a href="{{ BASE_PATH }}/pages/research.html"><img src="{{ BASE_PATH }}/pages/files/imitationset_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Vocal Imitation Set: a dataset of vocally imitated sound events using the AudioSet ontology]({{ BASE_PATH }}/pages/research.html)

The largest publicly-available datset of vocal imitations as well as the first to adopt the widely-used AudioSet ontology for a vocal imitation dataset. [[paper]]({{ BASE_PATH }}/pages/files/papers/DCASE2018Workshop_Kim_135.pdf) [[dataset]](https://zenodo.org/record/1340763)




<br>
<a href="{{ BASE_PATH }}/pages/research.html"><img src="{{ BASE_PATH }}/pages/files/lossy_compression_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Lossy Audio Compression Identification]({{ BASE_PATH }}/pages/research.html)

We can identify the lossy audio compression formats (e.g., WMA, MP3, AAC etc) only by analying a compressed audio signal without any metadata 

<br>

<br>
<a href="{{ BASE_PATH }}/pages/projects/socialeq.html"><img src="{{ BASE_PATH }}/pages/files/socialeq_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Speeding Learning of Personalized Audio Equalization]({{ BASE_PATH }}/pages/projects/socialeq.html)

Just rate how much you like audio examples provided by the machine. It will give you the equalization curve of your sound contept. [[read more]]({{ BASE_PATH }}/pages/projects/socialeq.html)

<br>
<a href="{{ BASE_PATH }}/pages/projects/markov.html"><img src="{{ BASE_PATH }}/pages/files/MC_improviosr_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Probabilistic Prediction of Rhythmic Characteristics in Markov Chain-based Melodic Sequences]({{ BASE_PATH }}/pages/projects/markov.html)

A Markov Chain-based interactive improvisation system which allows a user to control the level of syncopation and rhythmic tension in real-time. [[read more]]({{ BASE_PATH }}/pages/projects/markov.html)

<br>
<a href="{{ BASE_PATH }}/pages/projects/iam_hear.html"><img src="{{ BASE_PATH }}/pages/files/iamhear_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [I.M.Hear]({{ BASE_PATH }}/pages/projects/iam_hear.html)

I.M.Hear is a tabletop interface that uses mobile devices as UI components. Position of each mobile device on the table is tracked down in real time only using acoustic signals in "theoretically audible, but practically inaudible range. [[read more]]({{ BASE_PATH }}/pages/projects/iam_hear.html)

<br>
<a href="{{ BASE_PATH }}/pages/projects/ways.html"><img src="{{ BASE_PATH }}/pages/files/WAYS_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Where Are You Standing?]({{ BASE_PATH }}/pages/projects/ways.html)

**Where Are You Standing?** is an interactive, collaborative mobile music piece using smartphones' digital compass. [[read more]]({{ BASE_PATH }}/pages/projects/ways.html)<br><br><br>

<br>
<a href="{{ BASE_PATH }}/pages/projects/turning_into_sound.html"><img src="{{ BASE_PATH }}/pages/files/turningIntoSound_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Turning Into Sound]({{ BASE_PATH }}/pages/projects/turning_into_sound.html)

**Turning Into Sound** is an interactive multimedia installation. People can make music by drawing on a white board with three color pens. [[read more]]({{ BASE_PATH }}/pages/projects/turning_into_sound.html)<br><br><br>

<br>
<a href="{{ BASE_PATH }}/pages/projects/dont_imagine.html"><img src="{{ BASE_PATH }}/pages/files/dont_imagine_icon.png" width="35%" height="35%" align="left" style="margin:10px 20px "></a>
#### [Dance performance with mobile phone-Don't Imagine]({{ BASE_PATH }}/pages/projects/dont_imagine.html)

**Don't Imagine** is  a new media dance performance. I participated as a multimedia engineer to develop mobile-phone based interactive music controllers for dancers. [[read more]]({{ BASE_PATH }}/pages/projects/dont_imagine.html)

<!-- We describe a novel weakly labeled Audio Event Classification approach based on a self-supervised attention model. The weakly labeled framework is used to eliminate the need for expensive data labeling procedure and self-supervised attention is deployed to help a model distinguish between relevant and irrelevant parts of a weakly labeled audio clip in a more effective manner compared to prior attention models. We also propose a highly effective strongly supervised attention model when strong labels are available. This model also serves as an upper bound for the self-supervised model. The performances of the model with self-supervised attention training are comparable to the strongly supervised one which is trained using strong labels. We show that our self-supervised attention method is especially beneficial for short audio events.* -->

<!-- [Read more]({{ BASE_PATH }}/pages/research.html)
 -->
<br/>
<hr>
